featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Visualisation of Data and Exploratory Analyses
# Sample clustering and Principal Components Analysis
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
jpeg(filename= "/OneDrive/MSc/pheatmap.jpeg", units = "px", width = 4000, height = 2451, res = 300)
pheatmap(corMatrix,annotation_col=sampleInfo)
dev.off()
}
gse <- get_file_name('GDS859.soft.gz')
get_file_name <- function(filename) {
gse <- getGEO(filename=filename, destdir= "GDS Data Analysis")    # change my_id to the required dataset
#gse <- getGEO(filename='GDS859.soft.gz', destdir=".")
# check how many platforms used
length(gse)
# if more than one dataset is present, can analyse the other dataset by changing the number inside the [[...]]
# e.g. gds <- gds[[2]]
# Turning GDS object into an expression set object (using base 2 logarithms) and examining it
eset <- GDS2eSet(gse, do.log2=TRUE)
sampleNames(eset)
featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Visualisation of Data and Exploratory Analyses
# Sample clustering and Principal Components Analysis
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
jpeg(filename= "OneDrive/MSc/pheatmap.jpeg", units = "px", width = 4000, height = 2451, res = 300)
pheatmap(corMatrix,annotation_col=sampleInfo)
dev.off()
}
gse <- get_file_name('GDS859.soft.gz')
get_file_name <- function(filename) {
gse <- getGEO(filename=filename, destdir= "GDS Data Analysis")    # change my_id to the required dataset
#gse <- getGEO(filename='GDS859.soft.gz', destdir=".")
# check how many platforms used
length(gse)
# if more than one dataset is present, can analyse the other dataset by changing the number inside the [[...]]
# e.g. gds <- gds[[2]]
# Turning GDS object into an expression set object (using base 2 logarithms) and examining it
eset <- GDS2eSet(gse, do.log2=TRUE)
sampleNames(eset)
featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Visualisation of Data and Exploratory Analyses
# Sample clustering and Principal Components Analysis
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
jpeg(filename= "s3://elasticbeanstalk-eu-west-2-873965007921/visuals/pheatmap.jpeg", units = "px", width = 4000, height = 2451, res = 300)
pheatmap(corMatrix,annotation_col=sampleInfo)
dev.off()
}
gse <- get_file_name('GDS859.soft.gz')
# Importing the data
library(GEOquery)
detach("package:GEOquery", unload = TRUE)
library(GEOquery)
library(dplyr)
# Loading the required packages and dependencies
library("BiocManager")
library("forcats")
library("stringr")
library("limma")
#library(dpylr)
library(GEOquery)
library(pheatmap)
library(plsgenomics)
rm(list = ls()) # clear the R environment
get_file_name <- function(filename) {
gse <- getGEO(filename=filename, destdir= "GDS Data Analysis")    # change my_id to the required dataset
#gse <- getGEO(filename='GDS859.soft.gz', destdir=".")
# check how many platforms used
length(gse)
# if more than one dataset is present, can analyse the other dataset by changing the number inside the [[...]]
# e.g. gds <- gds[[2]]
# Turning GDS object into an expression set object (using base 2 logarithms) and examining it
eset <- GDS2eSet(gse, do.log2=TRUE)
sampleNames(eset)
featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Visualisation of Data and Exploratory Analyses
# Sample clustering and Principal Components Analysis
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
filepath1 = paste('static/visuals/',filename,'pheatmap.jpeg')
filepath_for_html = paste('visuals/',filename,'pheatmap.jpeg')
jpeg(filename= filepath, units = "px", width = 4000, height = 2451, res = 300)
pheatmap(corMatrix,annotation_col=sampleInfo)
dev.off()
return(filepath_for_html)
}
gse <- getGEO(filename=filename, destdir= "GDS Data Analysis")    # change my_id to the required dataset
gse <- getGEO(filename="GDS859.soft.gz", destdir= "GDS Data Analysis")    # change my_id to the required dataset
#gse <- getGEO(filename='GDS859.soft.gz', destdir=".")
# check how many platforms used
length(gse)
# Turning GDS object into an expression set object (using base 2 logarithms) and examining it
eset <- GDS2eSet(gse, do.log2=TRUE)
sampleNames(eset)
featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
filepath1 = paste('static/visuals/',filename,'pheatmap.jpeg')
filename = 'GDS859.soft.gz'
gse <- getGEO(filename=filename, destdir= "GDS Data Analysis")    # change my_id to the required dataset
#gse <- getGEO(filename='GDS859.soft.gz', destdir=".")
# check how many platforms used
length(gse)
# Turning GDS object into an expression set object (using base 2 logarithms) and examining it
eset <- GDS2eSet(gse, do.log2=TRUE)
sampleNames(eset)
featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
filepath1 = paste('static/visuals/',filena,'pheatmap.jpeg')
filepath_for_html = paste('visuals/',filename,'pheatmap.jpeg')
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
filepath1 = paste('static/visuals/',filename,'pheatmap.jpeg')
filepath1
filepath1 = paste('static/visuals/',filename,'pheatmap.jpeg',sep ='')
filepath1
filepath_for_html = paste('visuals/',filename,'pheatmap.jpeg',sep ='')
jpeg(filename= filepath, units = "px", width = 4000, height = 2451, res = 300)
jpeg(filename= filepath1, units = "px", width = 4000, height = 2451, res = 300)
rm(list = ls()) # clear the R environment
gse <- getGEO(filename=filename, destdir= "GDS Data Analysis")    # change my_id to the required dataset
rm(list = ls()) # clear the R environment
gse <- getGEO(filename=filename)    # change my_id to the required dataset
get_file_name('GDS589.soft.gz')
get_file_name <- function(filename) {
rm(list = ls()) # clear the R environment
gse <- getGEO(filename=filename)    # change my_id to the required dataset
#gse <- getGEO(filename='GDS859.soft.gz', destdir=".")
# check how many platforms used
length(gse)
# if more than one dataset is present, can analyse the other dataset by changing the number inside the [[...]]
# e.g. gds <- gds[[2]]
# Turning GDS object into an expression set object (using base 2 logarithms) and examining it
eset <- GDS2eSet(gse, do.log2=TRUE)
sampleNames(eset)
featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Visualisation of Data and Exploratory Analyses
# Sample clustering and Principal Components Analysis
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
jpeg(filename= '/static/visuals/pheatmap.jpeg', units = "px", width = 4000, height = 2451, res = 300)
pheatmap(corMatrix,annotation_col=sampleInfo)
dev.off()
}
get_file_name('GDS589.soft.gz')
get_file_name('GDS859.soft.gz')
n <- "GDS859.soft.gz"
get_file_name(n)
get_file_name <- function(filename) {
rm(list = ls()) # clear the R environment
gse <- getGEO(filename=filename, destdir=".")    # change my_id to the required dataset
#gse <- getGEO(filename='GDS859.soft.gz', destdir=".")
# check how many platforms used
length(gse)
# if more than one dataset is present, can analyse the other dataset by changing the number inside the [[...]]
# e.g. gds <- gds[[2]]
# Turning GDS object into an expression set object (using base 2 logarithms) and examining it
eset <- GDS2eSet(gse, do.log2=TRUE)
sampleNames(eset)
featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Visualisation of Data and Exploratory Analyses
# Sample clustering and Principal Components Analysis
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
jpeg(filename= '/static/visuals/pheatmap.jpeg', units = "px", width = 4000, height = 2451, res = 300)
pheatmap(corMatrix,annotation_col=sampleInfo)
dev.off()
}
n <- "GDS859.soft.gz"
get_file_name(n)
get_file_name <- function() {
rm(list = ls()) # clear the R environment
gse <- getGEO(filename='GDS859.soft.gz', destdir=".")    # change my_id to the required dataset
#gse <- getGEO(filename='GDS859.soft.gz', destdir=".")
# check how many platforms used
length(gse)
# if more than one dataset is present, can analyse the other dataset by changing the number inside the [[...]]
# e.g. gds <- gds[[2]]
# Turning GDS object into an expression set object (using base 2 logarithms) and examining it
eset <- GDS2eSet(gse, do.log2=TRUE)
sampleNames(eset)
featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Visualisation of Data and Exploratory Analyses
# Sample clustering and Principal Components Analysis
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
jpeg(filename= '/static/visuals/pheatmap.jpeg', units = "px", width = 4000, height = 2451, res = 300)
pheatmap(corMatrix,annotation_col=sampleInfo)
dev.off()
}
n <- "GDS859.soft.gz"
get_file_name(n)
get_file_name()
get_file_name <- function() {
rm(list = ls()) # clear the R environment
gse <- getGEO(filename='GDS859.soft.gz', destdir=".")    # change my_id to the required dataset
#gse <- getGEO(filename='GDS859.soft.gz', destdir=".")
# check how many platforms used
length(gse)
# if more than one dataset is present, can analyse the other dataset by changing the number inside the [[...]]
# e.g. gds <- gds[[2]]
# Turning GDS object into an expression set object (using base 2 logarithms) and examining it
eset <- GDS2eSet(gse, do.log2=TRUE)
sampleNames(eset)
featureData(eset)
# Check the normalisation and the scales used
## exprs get the expression levels as a data frame and get the distribution
summary(exprs(eset))
# Inspect the clinical variables
sampleInfo <- pData(eset)
# Visualisation of Data and Exploratory Analyses
# Sample clustering and Principal Components Analysis
# Heatmap
corMatrix <- cor(exprs(eset),use="c")
# Print the rownames of the sample information and check it matches the correlation matrix
rownames(sampleInfo)
colnames(corMatrix)
# Ensure rownames match the columns
rownames(sampleInfo) <- colnames(corMatrix)
jpeg(filename= 'pheatmap.jpeg', units = "px", width = 4000, height = 2451, res = 300)
pheatmap(corMatrix,annotation_col=sampleInfo)
dev.off()
}
n <- "GDS859.soft.gz"
get_file_name()
dev.off()
get_file_name()
# for RA results
relative_activity <- function() {
connec <- read.csv(file = 'connec_data.csv',header = FALSE)
connec2 <- read.csv(file = 'connec_data.csv')
connec <- data.matrix(connec, rownames.force = NA)
connec <- as.matrix(connec)
ge <- read.csv(file = 'ge_data.csv', header = FALSE)
ge2 <- read.csv(file = 'ge_data.csv')
ge <- data.matrix(ge, rownames.force = NA)
ge <- as.matrix(ge)
new <- TFA.estimate(CONNECdata = connec, GEdata = ge,ncomp=3,nruncv=0)
TFAc <- new$TFA
colnames(TFAc) <- colnames(ge2)
rownames(TFAc) <- colnames(connec2)
TFAc[is.nan(TFAc)] = 0
TFAc <- TFAc[,-1]
TFAc <- TFAc[-1,]
TFAc
#TFAc <- TFAc[ order(rowMeans(TFAc), decreasing = T), ] #doesn't work with datatables
# Add a new column for the average relative activity
TFAc <- cbind(TFAc, rowMeans(TFAc))
# Rename column as average
colnames(TFAc)[ncol(TFAc)] <- "AVERAGE"
write.csv(TFAc, "relative.csv")
}
relative_activity('GDS859.soft.gz')
library(readr)
connec_data <- read_csv("OneDrive/MSc/ach-testing/connec_data.csv")
View(connec_data)
setwd("~/OneDrive/MSc/ach-testing")
# for RA results
relative_activity <- function() {
connec <- read.csv(file = 'connec_data.csv',header = FALSE)
connec2 <- read.csv(file = 'connec_data.csv')
connec <- data.matrix(connec, rownames.force = NA)
connec <- as.matrix(connec)
ge <- read.csv(file = 'ge_data.csv', header = FALSE)
ge2 <- read.csv(file = 'ge_data.csv')
ge <- data.matrix(ge, rownames.force = NA)
ge <- as.matrix(ge)
new <- TFA.estimate(CONNECdata = connec, GEdata = ge,ncomp=3,nruncv=0)
TFAc <- new$TFA
colnames(TFAc) <- colnames(ge2)
rownames(TFAc) <- colnames(connec2)
TFAc[is.nan(TFAc)] = 0
TFAc <- TFAc[,-1]
TFAc <- TFAc[-1,]
TFAc
#TFAc <- TFAc[ order(rowMeans(TFAc), decreasing = T), ] #doesn't work with datatables
# Add a new column for the average relative activity
TFAc <- cbind(TFAc, rowMeans(TFAc))
# Rename column as average
colnames(TFAc)[ncol(TFAc)] <- "AVERAGE"
write.csv(TFAc, "relative.csv")
}
relative_activity()
library(plsgenomics)
relative_activity()
# for RA results
relative_activity <- function() {
connec <- read.csv(file = 'connec_data.csv',header = FALSE)
connec2 <- read.csv(file = 'connec_data.csv')
connec <- data.matrix(connec, rownames.force = NA)
connec <- as.matrix(connec)
ge <- read.csv(file = 'ge_data.csv', header = FALSE)
ge2 <- read.csv(file = 'ge_data.csv')
ge <- data.matrix(ge, rownames.force = NA)
ge <- as.matrix(ge)
new <- TFA.estimate(CONNECdata = connec, GEdata = ge,ncomp=3,nruncv=0)
TFAc <- new$TFA
colnames(TFAc) <- colnames(ge2)
rownames(TFAc) <- colnames(connec2)
TFAc[is.nan(TFAc)] = 0
TFAc <- TFAc[,-1]
TFAc <- TFAc[-1,]
TFAc
#TFAc <- TFAc[ order(rowMeans(TFAc), decreasing = T), ] #doesn't work with datatables
# Add a new column for the average relative activity
#TFAc <- cbind(TFAc, rowMeans(TFAc))
# Rename column as average
#colnames(TFAc)[ncol(TFAc)] <- "AVERAGE"
write.csv(TFAc, "relative.csv")
}
clear_env <- function() {
rm(list = ls()) # clear the R environment
}
relative_activity()
library(readr)
connec_data <- read_csv("connec_data.csv")
View(connec_data)
# for RA results
relative_activity <- function() {
connec <- read.csv(file = 'connec_data.csv',header = FALSE)
connec2 <- read.csv(file = 'connec_data.csv')
connec <- data.matrix(connec, rownames.force = NA)
connec <- as.matrix(connec)
ge <- read.csv(file = 'ge_data.csv', header = FALSE)
ge2 <- read.csv(file = 'ge_data.csv')
ge <- data.matrix(ge, rownames.force = NA)
ge <- as.matrix(ge)
new <- TFA.estimate(CONNECdata = connec, GEdata = ge,ncomp=3,nruncv=0)
TFAc <- new$TFA
colnames(TFAc) <- colnames(ge2)
rownames(TFAc) <- colnames(connec2)
TFAc[is.nan(TFAc)] = 0
TFAc <- TFAc[,-1]
TFAc <- TFAc[-1,]
TFAc
#TFAc <- TFAc[ order(rowMeans(TFAc), decreasing = T), ] #doesn't work with datatables
# Add a new column for the average relative activity
#TFAc <- cbind(TFAc, rowMeans(TFAc))
# Rename column as average
#colnames(TFAc)[ncol(TFAc)] <- "AVERAGE"
write.csv(TFAc, "relative.csv")
}
relative_activity()
# for RA results
relative_activity <- function() {
connec <- read.csv(file = 'connec_data.csv',header = FALSE, fileEncoding="latin1")
connec2 <- read.csv(file = 'connec_data.csv')
connec <- data.matrix(connec, rownames.force = NA)
connec <- as.matrix(connec)
ge <- read.csv(file = 'ge_data.csv', header = FALSE, fileEncoding="latin1")
ge2 <- read.csv(file = 'ge_data.csv')
ge <- data.matrix(ge, rownames.force = NA)
ge <- as.matrix(ge)
new <- TFA.estimate(CONNECdata = connec, GEdata = ge,ncomp=3,nruncv=0)
TFAc <- new$TFA
colnames(TFAc) <- colnames(ge2)
rownames(TFAc) <- colnames(connec2)
TFAc[is.nan(TFAc)] = 0
TFAc <- TFAc[,-1]
TFAc <- TFAc[-1,]
TFAc
#TFAc <- TFAc[ order(rowMeans(TFAc), decreasing = T), ] #doesn't work with datatables
# Add a new column for the average relative activity
#TFAc <- cbind(TFAc, rowMeans(TFAc))
# Rename column as average
#colnames(TFAc)[ncol(TFAc)] <- "AVERAGE"
write.csv(TFAc, "relative.csv")
}
relative_activity()
# for RA results
relative_activity <- function() {
connec <- read.csv(file = 'connec_data.csv',header = FALSE, fileEncoding="latin1")
connec2 <- read.csv(file = 'connec_data.csv')
connec <- data.matrix(connec, rownames.force = NA)
connec <- as.matrix(connec)
ge <- read.csv(file = 'ge_data.csv', header = FALSE, fileEncoding="latin1")
ge2 <- read.csv(file = 'ge_data.csv')
ge <- data.matrix(ge, rownames.force = NA)
ge <- as.matrix(ge)
new <- TFA.estimate(CONNECdata = connec, GEdata = ge,ncomp=3,nruncv=0)
TFAc <- new$TFA
colnames(TFAc) <- colnames(ge2)
rownames(TFAc) <- colnames(connec2)
TFAc[is.nan(TFAc)] = 0
TFAc <- TFAc[,-1]
TFAc <- TFAc[-1,]
TFAc
#TFAc <- TFAc[ order(rowMeans(TFAc), decreasing = T), ] #doesn't work with datatables
# Add a new column for the average relative activity
#TFAc <- cbind(TFAc, rowMeans(TFAc))
# Rename column as average
#colnames(TFAc)[ncol(TFAc)] <- "AVERAGE"
write.csv(TFAc, "relative.csv")
}
# for RA results
relative_activity <- function() {
connec <- read.csv(file = 'connec_data.csv',header = FALSE)
connec2 <- read.csv(file = 'connec_data.csv')
connec <- data.matrix(connec, rownames.force = NA)
connec <- as.matrix(connec)
ge <- read.csv(file = 'ge_data.csv', header = FALSE)
ge2 <- read.csv(file = 'ge_data.csv')
ge <- data.matrix(ge, rownames.force = NA)
ge <- as.matrix(ge)
new <- TFA.estimate(CONNECdata = connec, GEdata = ge,ncomp=3,nruncv=0)
TFAc <- new$TFA
colnames(TFAc) <- colnames(ge2)
rownames(TFAc) <- colnames(connec2)
TFAc[is.nan(TFAc)] = 0
TFAc <- TFAc[,-1]
TFAc <- TFAc[-1,]
TFAc
#TFAc <- TFAc[ order(rowMeans(TFAc), decreasing = T), ] #doesn't work with datatables
# Add a new column for the average relative activity
#TFAc <- cbind(TFAc, rowMeans(TFAc))
# Rename column as average
#colnames(TFAc)[ncol(TFAc)] <- "AVERAGE"
write.csv(TFAc, "relative.csv")
}
relative_activity()
